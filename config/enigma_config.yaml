# Enigma Configuration
# Last Updated: 2026-01-10

# Ollama Settings
ollama:
  base_url: http://localhost:11434

  # Model Selection
  # Current: gemma2:2b (fastest, basic intelligence)
  # Available alternatives:
  #   - qwen3:4b (better quality, slower)
  #   - qwen3:8b (high quality, much slower on 16GB RAM)
  #   - gemma3:1b (extremely fast but lower quality)
  model: gemma2:2b

  # Generation Settings
  temperature: 0.3       # Lower = faster, more focused (0.0 to 1.0)
  num_predict: 800       # Max tokens in response (lower = faster)
  top_p: 0.9            # Nucleus sampling
  top_k: 40             # Token selection diversity

# ChromaDB Settings
chromadb:
  persist_directory: ./data/chromadb
  collection_name: enigma_knowledge

  # Search Settings
  n_results: 3          # Number of chunks to retrieve (lower = faster)

# Folder Paths (relative to Enigma root)
folders:
  ground_truth: 99_GroundTruth
  systems: 02_Systems
  projects: 03_Projects
  memory: 04_Memory
  tasks: 05_Tasks
  derived: 06_Derived

# Logging
logging:
  level: INFO           # DEBUG | INFO | WARNING | ERROR
  file: logs/enigma.log
  console: true

# Document Processing
documents:
  chunk_size: 1000      # Words per chunk
  chunk_overlap: 200    # Overlap between chunks

  # Supported formats
  formats:
    - md
    - txt
    - docx
    - pdf
    - pptx

# Performance Notes
# ==================
# Model Choice based on Mac M4 16GB RAM:
# - gemma2:2b:  5-15 sec responses (CURRENT - best for speed)
# - qwen3:4b:   15-45 sec responses (better quality, acceptable speed)
# - qwen3:8b:   60-120 sec responses (slow on 16GB)
# - qwen3:30b:  120-180 sec responses (too slow for 16GB)
#
# For deeper intelligence tasks, use external Claude/GPT APIs
# Enigma focuses on: quick queries, document citation, basic automation
